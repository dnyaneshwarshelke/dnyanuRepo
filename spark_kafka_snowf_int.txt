# Spark _Kafka _Snowflake Intergration

# Read data from dstream kafka 

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.types import *

import re

spark=SparkSession.builder.master("local[*]").appName("test").getOrCreate()

df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092")\
.option("subscribe", "mar7") \
.load()
df.printSchema()
df1 = df.selectExpr("CAST(value AS STRING)")

#changing data type binary to string using CAST function

def read_nested_json(df):
  column_list = []
  for column_name in df.schema.names:
    if isinstance(df.schema[column_name].dataType, ArrayType):
      df = df.withColumn(column_name, explode(column_name))

      column_list.append(column_name)
    elif isinstance(df.schema[column_name].dataType, StructType):
      for field in df.schema[column_name].dataType.fields:
        column_list.append(col(column_name + "." + field.name).alias(column_name + "_" + field.name))
    else:
      column_list.append(column_name)
  df = df.select(column_list)
  return df;

def flatten(df):
  read_nested_json_flag = True
  while read_nested_json_flag:
    df = read_nested_json(df);
    read_nested_json_flag = False
    for column_name in df.schema.names:
      if isinstance(df.schema[column_name].dataType, ArrayType):
        read_nested_json_flag = True
      elif isinstance(df.schema[column_name].dataType, StructType):
        read_nested_json_flag = True;
  cols = [re.sub('[^a-zA-Z0-1]', "", c.lower()) for c in df.columns]
  return df.toDF(*cols);
 
#Write kafka stream data into snowflake Warehouse  

ndf = flatten(df1)
def foreach_batch_function(df, epoch_id):
  sfOptions = {
    "sfURL" : "tlwvpwa-ui22648.snowflakecomputing.com",
    "sfUser" : "CHINMAYSF",
    "sfPassword" : "SFpassword.1",
    "sfDatabase" : "chinmaydb",
    "sfSchema" : "public",
    "sfWarehouse" : "COMPUTE_WH"
  }
  SNOWFLAKE_SOURCE_NAME = "net.snowflake.spark.snowflake"
  df.write.format(SNOWFLAKE_SOURCE_NAME).options(**sfOptions).option("dbtable", "kalive").mode("append").save()

  pass
ndf.writeStream.foreachBatch(foreach_batch_function).start().awaitTermination()

#ndf.writeStream.outputMode("append").format("console").start().awaitTermination()